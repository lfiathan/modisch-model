{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1654,
     "status": "ok",
     "timestamp": 1760409026274,
     "user": {
      "displayName": "alif aditya",
      "userId": "09765680414298989716"
     },
     "user_tz": -420
    },
    "id": "kMi2042vPb_X",
    "outputId": "edcc49a4-d6bf-4af9-ee54-f7fa0b9d4c46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data source import complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import kaggle\n",
    "import yaml\n",
    "import pathlib\n",
    "import random\n",
    "import shutil\n",
    "import torch\n",
    "import sys\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#test pr\n",
    "# Notebook can't use relative package imports; ensure src is on sys.path and import absolutely.\n",
    "sys.path.append(str(pathlib.Path('..').resolve() / 'src'))\n",
    "from modeling.common import switch_labels\n",
    "\n",
    "print('Data source import complete.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3gAXwA1GPb_Y"
   },
   "source": [
    "# YOLOv8\n",
    "YOLOv8 is the newest state-of-the-art YOLO model that can be used for object detection, image classification, and instance segmentation tasks.<br>\n",
    "YOLOv8 includes numerous architectural and developer experience changes and improvements over YOLOv5.<br>\n",
    "\n",
    "## Why Should I Use YOLOv8?\n",
    "* YOLOv8 has a high rate of accuracy measured by COCO and Roboflow 100.\n",
    "* YOLOv8 comes with a lot of developer-convenience features, from an easy-to-use CLI to a well-structured Python package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths (match your notebook variables)\n",
    "output_path = '../dataset'\n",
    "dataset_root = os.path.join(output_path, 'colorful_fashion_dataset_for_object_detection')\n",
    "jpeg_dir = os.path.join(dataset_root, 'JPEGImages')\n",
    "images_alias = os.path.join(dataset_root, 'images')  # YOLO-friendly\n",
    "labels_dir = os.path.join(dataset_root, 'labels')    # where your phase labels are moved before training\n",
    "original_annotations_path = os.path.join(dataset_root, 'Annotations_txt')\n",
    "\n",
    "# 1) Make 'images' alias -> JPEGImages\n",
    "if not os.path.exists(images_alias):\n",
    "    try:\n",
    "        os.symlink(os.path.relpath(jpeg_dir, dataset_root), images_alias)\n",
    "        print(f\"✅ Symlink created: {images_alias} -> {jpeg_dir}\")\n",
    "    except Exception as e:\n",
    "        # Fallback: duplicate folder name (uses extra space)\n",
    "        if not os.path.exists(images_alias):\n",
    "            shutil.copytree(jpeg_dir, images_alias)\n",
    "            print(f\"✅ Copied images to: {images_alias}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Move phase labels to 'labels' directory\n",
    "def move_dir_overwrite(src, dst):\n",
    "    if not os.path.exists(src):\n",
    "        print(f\"Skip move: missing {src}\")\n",
    "        return\n",
    "    if os.path.exists(dst):\n",
    "        # avoid rmtree if they are the same inode\n",
    "        try:\n",
    "            if os.path.samefile(src, dst):\n",
    "                return\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "        shutil.rmtree(dst)\n",
    "    shutil.move(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Wrote /Users/macm4/repositories/Machine Learning Model/modisch-model/dataset/colorful_fashion_dataset_for_object_detection/train_full.txt with 2145 entries\n",
      "✅ Wrote /Users/macm4/repositories/Machine Learning Model/modisch-model/dataset/colorful_fashion_dataset_for_object_detection/val_full.txt with 537 entries\n",
      "✅ Wrote phase1-data.yaml\n",
      "✅ Wrote phase2-data.yaml\n",
      "Done. Now ensure labels_dir points to the correct phase before training.\n"
     ]
    }
   ],
   "source": [
    "# 2) Regenerate train/val lists that point to the 'images' alias\n",
    "def resolve_image_path(img_name, images_dir):\n",
    "    img_name = img_name.strip()\n",
    "    if not img_name:\n",
    "        return None\n",
    "    exts = ('.jpg', '.jpeg', '.png', '.JPG', '.PNG')\n",
    "    cands = [img_name] if os.path.splitext(img_name)[1] else [img_name + ext for ext in exts]\n",
    "    for c in cands:\n",
    "        p = os.path.join(images_dir, c)\n",
    "        if os.path.exists(p):\n",
    "            return os.path.abspath(p)\n",
    "    base = os.path.splitext(img_name)[0]\n",
    "    for f in os.listdir(images_dir):\n",
    "        if os.path.splitext(f)[0] == base:\n",
    "            return os.path.abspath(os.path.join(images_dir, f))\n",
    "    return None\n",
    "\n",
    "def make_list_from_imagesets(images_dir, list_relpath, out_txt):\n",
    "    list_path = os.path.join(dataset_root, list_relpath)\n",
    "    lines = []\n",
    "    if os.path.exists(list_path):\n",
    "        with open(list_path, 'r') as f:\n",
    "            for l in f:\n",
    "                p = resolve_image_path(l, images_dir)\n",
    "                if p:\n",
    "                    lines.append(p)\n",
    "    else:\n",
    "        print(f\"⚠️ Missing: {list_path}\")\n",
    "    with open(out_txt, 'w') as f:\n",
    "        f.write(\"\\n\".join(lines))\n",
    "    print(f\"✅ Wrote {out_txt} with {len(lines)} entries\")\n",
    "\n",
    "train_list_txt = os.path.abspath(os.path.join(dataset_root, 'train_full.txt'))\n",
    "val_list_txt   = os.path.abspath(os.path.join(dataset_root, 'val_full.txt'))\n",
    "\n",
    "make_list_from_imagesets(images_alias, 'ImageSets/Main/trainval.txt', train_list_txt)\n",
    "make_list_from_imagesets(images_alias, 'ImageSets/Main/test.txt',     val_list_txt)\n",
    "\n",
    "# 3) Rewrite YAMLs to reference these lists\n",
    "def write_phase_yaml(path_root, train_txt, val_txt, names, out_fname):\n",
    "    cfg = {\n",
    "        'path': os.path.abspath(path_root),\n",
    "        'train': os.path.abspath(train_txt),\n",
    "        'val': os.path.abspath(val_txt),\n",
    "        'names': {i: n for i, n in enumerate(names)},\n",
    "    }\n",
    "    with open(out_fname, 'w') as f:\n",
    "        yaml.dump(cfg, f, sort_keys=False)\n",
    "    print(f\"✅ Wrote {out_fname}\")\n",
    "\n",
    "phase1_names = ['TOP','BOTTOM','SHOES']\n",
    "phase2_names = ['jacket','shirt','pants','shorts','skirt','dress','shoe']\n",
    "\n",
    "write_phase_yaml(dataset_root, train_list_txt, val_list_txt, phase1_names, 'phase1-data.yaml')\n",
    "write_phase_yaml(dataset_root, train_list_txt, val_list_txt, phase2_names, 'phase2-data.yaml')\n",
    "\n",
    "# 4) Clear stale caches so YOLO reindexes with labels\n",
    "for cache_name in ('JPEGImages.cache', 'images.cache'):\n",
    "    cp = os.path.join(dataset_root, cache_name)\n",
    "    if os.path.exists(cp):\n",
    "        os.remove(cp)\n",
    "        print(f\"🗑️ Removed cache: {cp}\")\n",
    "\n",
    "print(\"Done. Now ensure labels_dir points to the correct phase before training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original classes from the downloaded dataset's labels.txt\n",
    "original_classes = {\n",
    "    0: 'sunglass', 1: 'hat', 2: 'jacket', 3: 'shirt', 4: 'pants',\n",
    "    5: 'shorts', 6: 'skirt', 7: 'dress', 8: 'bag', 9: 'shoe'\n",
    "}\n",
    "\n",
    "# PHASE 1: New coarse classes (TOP, BOTTOM, SHOES)\n",
    "phase1_classes = {'TOP': 0, 'BOTTOM': 1, 'SHOES': 2}\n",
    "# This maps the ORIGINAL class ID to the NEW Phase 1 class ID\n",
    "phase1_map = {\n",
    "    0: None,  # sunglass\n",
    "    1: None,  # hat\n",
    "    2: 0,     # jacket -> TOP\n",
    "    3: 0,     # shirt  -> TOP\n",
    "    4: 1,     # pants  -> BOTTOM\n",
    "    5: 1,     # shorts -> BOTTOM\n",
    "    6: 1,     # skirt  -> BOTTOM\n",
    "    7: 0,     # dress  -> TOP\n",
    "    8: None,  # bag\n",
    "    9: 2      # shoe   -> SHOES\n",
    "}\n",
    "\n",
    "# PHASE 2: Your new fine-grained classes (without sunglass, bag, hat)\n",
    "phase2_classes = {\n",
    "    'jacket': 0, 'shirt': 1, 'pants': 2, 'shorts': 3,\n",
    "    'skirt': 4, 'dress': 5, 'shoe': 6\n",
    "}\n",
    "# This maps the ORIGINAL class ID to the NEW Phase 2 class ID\n",
    "phase2_map = {\n",
    "    0: None,  # sunglass\n",
    "    1: None,  # hat\n",
    "    2: 0,     # jacket\n",
    "    3: 1,     # shirt\n",
    "    4: 2,     # pants\n",
    "    5: 3,     # shorts\n",
    "    6: 4,     # skirt\n",
    "    7: 5,     # dress\n",
    "    8: None,  # bag\n",
    "    9: 6      # shoe\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_phase1_dir = os.path.join(dataset_root, 'labels_phase1')\n",
    "labels_phase2_dir = os.path.join(dataset_root, 'labels_phase2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Create the new label directories ---\n",
    "# Clear any old directories first and create clean ones\n",
    "if os.path.exists(labels_phase1_dir): shutil.rmtree(labels_phase1_dir)\n",
    "os.makedirs(labels_phase1_dir)\n",
    "\n",
    "if os.path.exists(labels_phase2_dir): shutil.rmtree(labels_phase2_dir)\n",
    "os.makedirs(labels_phase2_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Automatically processed 2683 files.\n",
      "-> Phase 1 labels are ready in: '../dataset/colorful_fashion_dataset_for_object_detection/labels_phase1'\n",
      "-> Phase 2 labels are ready in: '../dataset/colorful_fashion_dataset_for_object_detection/labels_phase2'\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Process each original label file ---\n",
    "processed_files = 0\n",
    "for filename in os.listdir(original_annotations_path):\n",
    "    if not filename.endswith('.txt'):\n",
    "        continue\n",
    "\n",
    "    new_phase1_lines = []\n",
    "    new_phase2_lines = []\n",
    "\n",
    "    with open(os.path.join(original_annotations_path, filename), 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if not parts:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                original_class_id = int(parts[0])\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "            box_coords = ' '.join(parts[1:])\n",
    "\n",
    "            # --- Process for Phase 1 ---\n",
    "            if original_class_id in phase1_map:\n",
    "                new_class_id = phase1_map[original_class_id]\n",
    "                if new_class_id is not None:\n",
    "                    new_phase1_lines.append(f\"{new_class_id} {box_coords}\")\n",
    "\n",
    "            # --- Process for Phase 2 ---\n",
    "            if original_class_id in phase2_map:\n",
    "                new_class_id = phase2_map[original_class_id]\n",
    "                if new_class_id is not None:\n",
    "                    new_phase2_lines.append(f\"{new_class_id} {box_coords}\")\n",
    "\n",
    "    # Write the new, filtered label files for each phase (create empty file if nothing)\n",
    "    with open(os.path.join(labels_phase1_dir, filename), 'w') as f:\n",
    "        if new_phase1_lines:\n",
    "            f.write('\\n'.join(new_phase1_lines))\n",
    "\n",
    "    with open(os.path.join(labels_phase2_dir, filename), 'w') as f:\n",
    "        if new_phase2_lines:\n",
    "            f.write('\\n'.join(new_phase2_lines))\n",
    "\n",
    "    processed_files += 1\n",
    "\n",
    "print(f\"✅ Automatically processed {processed_files} files.\")\n",
    "print(f\"-> Phase 1 labels are ready in: '{labels_phase1_dir}'\")\n",
    "print(f\"-> Phase 2 labels are ready in: '{labels_phase2_dir}'\")\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Wrote /Users/macm4/repositories/Machine Learning Model/modisch-model/dataset/colorful_fashion_dataset_for_object_detection/train_full.txt with 2145 entries\n",
      "✅ Wrote /Users/macm4/repositories/Machine Learning Model/modisch-model/dataset/colorful_fashion_dataset_for_object_detection/val_full.txt with 537 entries\n",
      "✅ Wrote phase1-data.yaml\n",
      "✅ Wrote phase2-data.yaml\n",
      "✅ YAML configuration files created: 'phase1-data.yaml', 'phase2-data.yaml'.\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Create train/val image lists and YAML files for YOLOv8 ---\n",
    "def resolve_image_path(img_name, images_dir):\n",
    "    img_name = img_name.strip()\n",
    "    if not img_name:\n",
    "        return None\n",
    "    # if already has extension, check it directly\n",
    "    candidates = [img_name] if os.path.splitext(img_name)[1] else [img_name + ext for ext in ('.jpg', '.jpeg', '.png', '.png', '.JPG')]\n",
    "    for c in candidates:\n",
    "        p = os.path.join(images_dir, c)\n",
    "        if os.path.exists(p):\n",
    "            return os.path.abspath(p)\n",
    "    # try any file that startswith basename (some datasets have suffixes)\n",
    "    base = os.path.splitext(img_name)[0]\n",
    "    for f in os.listdir(images_dir):\n",
    "        if os.path.splitext(f)[0] == base:\n",
    "            return os.path.abspath(os.path.join(images_dir, f))\n",
    "    return None\n",
    "\n",
    "def make_list_from_imagesets(images_dir, list_relpath, out_txt):\n",
    "    list_path = os.path.join(dataset_root, list_relpath)\n",
    "    if not os.path.exists(list_path):\n",
    "        print(f\"⚠️  Missing file: {list_path}\")\n",
    "        open(out_txt, 'w').close()\n",
    "        return\n",
    "    lines = []\n",
    "    with open(list_path, 'r') as f:\n",
    "        for l in f:\n",
    "            p = resolve_image_path(l, images_dir)\n",
    "            if p:\n",
    "                lines.append(p)\n",
    "    if not lines:\n",
    "        print(f\"❗ No images resolved from {list_path}. Check filenames and extensions in {images_dir}.\")\n",
    "    with open(out_txt, 'w') as f:\n",
    "        f.write(\"\\n\".join(lines))\n",
    "    print(f\"✅ Wrote {out_txt} with {len(lines)} entries\")\n",
    "\n",
    "# image folder\n",
    "images_dir = jpeg_dir  # ./dataset/.../JPEGImages\n",
    "\n",
    "# create train/val txts with absolute image paths for Phase 1 / Phase 2 (they can reuse same lists)\n",
    "train_list_txt = os.path.abspath(os.path.join(dataset_root, 'train_full.txt'))\n",
    "val_list_txt   = os.path.abspath(os.path.join(dataset_root, 'val_full.txt'))\n",
    "\n",
    "# original ImageSets/Main files (VOC-style)\n",
    "make_list_from_imagesets(images_dir, 'ImageSets/Main/trainval.txt', train_list_txt)\n",
    "make_list_from_imagesets(images_dir, 'ImageSets/Main/test.txt', val_list_txt)\n",
    "\n",
    "# Helper to write yaml pointing to the list files\n",
    "def write_phase_yaml(path_root, train_txt, val_txt, names, out_fname):\n",
    "    cfg = {\n",
    "        'path': os.path.abspath(path_root),\n",
    "        'train': os.path.abspath(train_txt),\n",
    "        'val': os.path.abspath(val_txt),\n",
    "        'names': {i: n for i, n in enumerate(names)}\n",
    "    }\n",
    "    with open(out_fname, 'w') as f:\n",
    "        yaml.dump(cfg, f, sort_keys=False)\n",
    "    print(f\"✅ Wrote {out_fname}\")\n",
    "\n",
    "# Phase1 names list in index order\n",
    "phase1_names = [k for k,v in sorted(phase1_classes.items(), key=lambda kv: kv[1])]\n",
    "# Phase2 names list\n",
    "phase2_names = [k for k,v in sorted(phase2_classes.items(), key=lambda kv: kv[1])]\n",
    "\n",
    "write_phase_yaml(dataset_root, train_list_txt, val_list_txt, phase1_names, 'phase1-data.yaml')\n",
    "write_phase_yaml(dataset_root, train_list_txt, val_list_txt, phase2_names, 'phase2-data.yaml')\n",
    "print(\"✅ YAML configuration files created: 'phase1-data.yaml', 'phase2-data.yaml'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "train_full.txt: 2145 images, 2145 non-empty labels, 0 missing labels\n",
      "val_full.txt: 537 images, 537 non-empty labels, 0 missing labels\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1) Device detection (CUDA preferred, then MPS on Apple silicon, else CPU)\n",
    "device = 'cuda' if torch.cuda.is_available() else ('mps' if getattr(torch.backends, 'mps', None) and torch.backends.mps.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# 2) Quick label-coverage check (counts non-empty label files for train/val lists)\n",
    "def check_label_coverage(list_txt, labels_dir):\n",
    "    imgs = []\n",
    "    with open(list_txt, 'r') as f:\n",
    "        imgs = [l.strip() for l in f if l.strip()]\n",
    "    non_empty = 0\n",
    "    missing = 0\n",
    "    for p in imgs:\n",
    "        base = os.path.splitext(os.path.basename(p))[0]\n",
    "        lf = os.path.join(labels_dir, base + '.txt')\n",
    "        if not os.path.exists(lf):\n",
    "            missing += 1\n",
    "        else:\n",
    "            if os.path.getsize(lf) > 0:\n",
    "                non_empty += 1\n",
    "    print(f\"{os.path.basename(list_txt)}: {len(imgs)} images, {non_empty} non-empty labels, {missing} missing labels\")\n",
    "\n",
    "labels_dir = os.path.join(dataset_root, 'labels')  # this is what YOLO expects\n",
    "check_label_coverage(train_list_txt, labels_dir)\n",
    "check_label_coverage(val_list_txt, labels_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 🚀 STARTING PHASE 1 TRAINING ---\n",
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt': 100% ━━━━━━━━━━━━ 6.2MB 425.0KB/s 15.1s.0s<0.1s8.2s\n",
      "Ultralytics 8.3.217 🚀 Python-3.9.6 torch-2.8.0 MPS (Apple M4)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=True, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=phase1-data.yaml, degrees=0.0, deterministic=True, device=mps, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=True, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=512, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=yolov8_phase1_coarse, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=10, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/Users/macm4/repositories/Machine Learning Model/modisch-model/runs/detect/yolov8_phase1_coarse, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751897  ultralytics.nn.modules.head.Detect           [3, [64, 128, 256]]           \n",
      "Model summary: 129 layers, 3,011,433 parameters, 3,011,417 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 118.7±53.9 MB/s, size: 44.0 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/macm4/repositories/Machine Learning Model/modisch-model/dataset/colorful_fashion_dataset_for_object_detection/JPEGImages... 0 images, 2145 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 2145/2145 10.1Kit/s 0.2s.0s\n",
      "WARNING ⚠️ \u001b[34m\u001b[1mtrain: \u001b[0mNo labels found in /Users/macm4/repositories/Machine Learning Model/modisch-model/dataset/colorful_fashion_dataset_for_object_detection/JPEGImages.cache. See https://docs.ultralytics.com/datasets for dataset formatting guidance.\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /Users/macm4/repositories/Machine Learning Model/modisch-model/dataset/colorful_fashion_dataset_for_object_detection/JPEGImages.cache\n",
      "WARNING ⚠️ Labels are missing or empty in /Users/macm4/repositories/Machine Learning Model/modisch-model/dataset/colorful_fashion_dataset_for_object_detection/JPEGImages.cache, training may not work correctly. See https://docs.ultralytics.com/datasets for dataset formatting guidance.\n",
      "WARNING ⚠️ cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (1.0GB RAM): 100% ━━━━━━━━━━━━ 2145/2145 5.5Kit/s 0.4s0.1s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 242.2±86.5 MB/s, size: 40.2 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/macm4/repositories/Machine Learning Model/modisch-model/dataset/colorful_fashion_dataset_for_object_detection/JPEGImages... 0 images, 537 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 537/537 10.5Kit/s 0.1s\n",
      "WARNING ⚠️ \u001b[34m\u001b[1mval: \u001b[0mNo labels found in /Users/macm4/repositories/Machine Learning Model/modisch-model/dataset/colorful_fashion_dataset_for_object_detection/JPEGImages.cache. See https://docs.ultralytics.com/datasets for dataset formatting guidance.\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /Users/macm4/repositories/Machine Learning Model/modisch-model/dataset/colorful_fashion_dataset_for_object_detection/JPEGImages.cache\n",
      "WARNING ⚠️ Labels are missing or empty in /Users/macm4/repositories/Machine Learning Model/modisch-model/dataset/colorful_fashion_dataset_for_object_detection/JPEGImages.cache, training may not work correctly. See https://docs.ultralytics.com/datasets for dataset formatting guidance.\n",
      "WARNING ⚠️ cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.3GB RAM): 100% ━━━━━━━━━━━━ 537/537 5.4Kit/s 0.1s\n",
      "Plotting labels to /Users/macm4/repositories/Machine Learning Model/modisch-model/runs/detect/yolov8_phase1_coarse/labels.jpg... \n",
      "WARNING ⚠️ zero-size array to reduction operation maximum which has no identity\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001429, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 512 train, 512 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1m/Users/macm4/repositories/Machine Learning Model/modisch-model/runs/detect/yolov8_phase1_coarse\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       1/50      2.52G          0      62.35          0          0        512: 100% ━━━━━━━━━━━━ 135/135 2.4it/s 55.6s0.4ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 17/17 3.2it/s 5.4s0.3s\n",
      "                   all        537          0          0          0          0          0\n",
      "WARNING ⚠️ no labels found in detect set, can not compute metrics without labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macm4/repositories/Machine Learning Model/modisch-model/.venv/lib/python3.9/site-packages/ultralytics/utils/metrics.py:850: RuntimeWarning: Mean of empty slice.\n",
      "  i = smooth(f1_curve.mean(0), 0.1).argmax()  # max F1 index\n",
      "/Users/macm4/repositories/Machine Learning Model/modisch-model/.venv/lib/python3.9/site-packages/numpy/_core/_methods.py:130: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       2/50      3.31G          0       44.9          0          0        512: 39% ━━━━╸─────── 53/135 3.6it/s 21.7s<22.9s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m move_dir_overwrite(labels_phase1_dir, labels_dir)\n\u001b[1;32m      6\u001b[0m model_p1 \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myolov8n.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mmodel_p1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mphase1-data.yaml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# reduce if you want faster cycles\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# smaller images = faster\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# increase if you have CPU cores and fast I/O\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# use GPU/CUDA or MPS if available\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# cache images for faster epochs (ram/disk)\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhalf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;66;43;03m# use fp16 on GPU/MPS where supported\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# early stopping after 10 epochs without val improvement\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43myolov8_phase1_coarse\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- ✅ PHASE 1 TRAINING COMPLETE ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Put labels back\u001b[39;00m\n",
      "File \u001b[0;32m~/repositories/Machine Learning Model/modisch-model/.venv/lib/python3.9/site-packages/ultralytics/engine/model.py:800\u001b[0m, in \u001b[0;36mModel.train\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    797\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mget_model(weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, cfg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39myaml)\n\u001b[1;32m    798\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m--> 800\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m}:\n",
      "File \u001b[0;32m~/repositories/Machine Learning Model/modisch-model/.venv/lib/python3.9/site-packages/ultralytics/engine/trainer.py:238\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    235\u001b[0m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 238\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repositories/Machine Learning Model/modisch-model/.venv/lib/python3.9/site-packages/ultralytics/engine/trainer.py:422\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    420\u001b[0m     loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_items \u001b[38;5;241m=\u001b[39m unwrap_model(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\u001b[38;5;241m.\u001b[39mloss(batch, preds)\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 422\u001b[0m     loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_items \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/repositories/Machine Learning Model/modisch-model/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repositories/Machine Learning Model/modisch-model/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/repositories/Machine Learning Model/modisch-model/.venv/lib/python3.9/site-packages/ultralytics/nn/tasks.py:138\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;124;03mPerform forward pass of the model for either training or inference.\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m    (torch.Tensor): Loss if x is a dict (training), or network predictions (inference).\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/repositories/Machine Learning Model/modisch-model/.venv/lib/python3.9/site-packages/ultralytics/nn/tasks.py:339\u001b[0m, in \u001b[0;36mBaseModel.loss\u001b[0;34m(self, batch, preds)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m preds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 339\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repositories/Machine Learning Model/modisch-model/.venv/lib/python3.9/site-packages/ultralytics/utils/loss.py:258\u001b[0m, in \u001b[0;36mv8DetectionLoss.__call__\u001b[0;34m(self, preds, batch)\u001b[0m\n\u001b[1;32m    256\u001b[0m dtype \u001b[38;5;241m=\u001b[39m pred_scores\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    257\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m pred_scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 258\u001b[0m imgsz \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeats\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# image size (h,w)\u001b[39;00m\n\u001b[1;32m    259\u001b[0m anchor_points, stride_tensor \u001b[38;5;241m=\u001b[39m make_anchors(feats, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m    261\u001b[0m \u001b[38;5;66;03m# Targets\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 3) Train with GPU/MPS, caching and mixed precision + early stopping (lower patience)\n",
    "print(\"\\n--- 🚀 STARTING PHASE 1 TRAINING ---\")\n",
    "# ensure labels for phase1 are available at dataset_root/labels (you already rename earlier)\n",
    "move_dir_overwrite(labels_phase1_dir, labels_dir)\n",
    "\n",
    "model_p1 = YOLO('yolov8n.pt')\n",
    "model_p1.train(\n",
    "    data='phase1-data.yaml',\n",
    "    epochs=50,            # reduce if you want faster cycles\n",
    "    imgsz=512,            # smaller images = faster\n",
    "    batch=16,\n",
    "    workers=8,            # increase if you have CPU cores and fast I/O\n",
    "    device=device,        # use GPU/CUDA or MPS if available\n",
    "    cache=True,           # cache images for faster epochs (ram/disk)\n",
    "    half=(device != 'cpu'),# use fp16 on GPU/MPS where supported\n",
    "    patience=10,          # early stopping after 10 epochs without val improvement\n",
    "    name='yolov8_phase1_coarse',\n",
    "    exist_ok=True\n",
    ")\n",
    "print(\"--- ✅ PHASE 1 TRAINING COMPLETE ---\")\n",
    "# Put labels back\n",
    "move_dir_overwrite(labels_dir, labels_phase1_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before Phase 2 training\n",
    "\n",
    "move_dir_overwrite(labels_phase2_dir, labels_dir)\n",
    "\n",
    "print(\"\\n--- 🚀 STARTING PHASE 2 TRAINING ---\")\n",
    "with switch_labels(labels_phase2_dir, labels_dir):\n",
    "    model_p2 = YOLO('runs/detect/yolov8_phase1_coarse/weights/best.pt')\n",
    "    model_p2.train(\n",
    "        data='phase2-data.yaml',\n",
    "        epochs=50, imgsz=512, batch=16, workers=8,\n",
    "        device=device, cache=True, half=(device!='cpu'),\n",
    "        patience=10, name='yolov8_phase2_fine', exist_ok=True\n",
    "    )\n",
    "\n",
    "print(\"--- ✅ PHASE 2 TRAINING COMPLETE ---\")\n",
    "move_dir_overwrite(labels_dir, labels_phase2_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GitHub Copilot\n",
    "\n",
    "Cause\n",
    "You’re renaming a directory onto an existing non-empty directory. On macOS, os.rename can’t overwrite a non-empty destination, and your cell calls rename twice (safe_rename + os.rename), making it more likely to fail.\n",
    "\n",
    "Quick fix in your notebook\n",
    "- Add this helper once.\n",
    "- Replace both os.rename(...) calls with move_dir_overwrite(...).\n",
    "- Remove the duplicate rename line.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add once in a notebook cell\n",
    "import os, shutil\n",
    "\n",
    "def move_dir_overwrite(src, dst):\n",
    "    if not os.path.exists(src):\n",
    "        print(f\"Skip move: missing {src}\")\n",
    "        return\n",
    "    if os.path.exists(dst):\n",
    "        # avoid rmtree if they are the same inode\n",
    "        try:\n",
    "            if os.path.samefile(src, dst):\n",
    "                return\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "        shutil.rmtree(dst)\n",
    "    shutil.move(src, dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Use it around training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before Phase 1 training\n",
    "move_dir_overwrite(labels_phase1_dir, labels_dir)\n",
    "\n",
    "# ... train Phase 1 ...\n",
    "\n",
    "print(\"--- ✅ PHASE 1 TRAINING COMPLETE ---\")\n",
    "# Put labels back\n",
    "move_dir_overwrite(labels_dir, labels_phase1_dir)\n",
    "\n",
    "# Before Phase 2 training\n",
    "move_dir_overwrite(labels_phase2_dir, labels_dir)\n",
    "\n",
    "# ... train Phase 2 ...\n",
    "\n",
    "print(\"--- ✅ PHASE 2 TRAINING COMPLETE ---\")\n",
    "move_dir_overwrite(labels_dir, labels_phase2_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Cleaner alternative\n",
    "Use your existing context manager to avoid manual moves entirely:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, pathlib\n",
    "sys.path.append(str(pathlib.Path('..').resolve() / 'src'))\n",
    "from modeling.common import switch_labels\n",
    "\n",
    "print(\"\\n--- 🚀 STARTING PHASE 1 TRAINING ---\")\n",
    "with switch_labels(labels_phase1_dir, labels_dir):\n",
    "    model_p1 = YOLO('yolov8n.pt')\n",
    "    model_p1.train(\n",
    "        data='phase1-data.yaml',\n",
    "        epochs=50, imgsz=512, batch=16, workers=8,\n",
    "        device=device, cache=True, half=(device!='cpu'),\n",
    "        patience=10, name='yolov8_phase1_coarse', exist_ok=True\n",
    "    )\n",
    "print(\"--- ✅ PHASE 1 TRAINING COMPLETE ---\")\n",
    "\n",
    "print(\"\\n--- 🚀 STARTING PHASE 2 TRAINING ---\")\n",
    "with switch_labels(labels_phase2_dir, labels_dir):\n",
    "    model_p2 = YOLO('runs/detect/yolov8_phase1_coarse/weights/best.pt')\n",
    "    model_p2.train(\n",
    "        data='phase2-data.yaml',\n",
    "        epochs=50, imgsz=512, batch=16, workers=8,\n",
    "        device=device, cache=True, half=(device!='cpu'),\n",
    "        patience=10, name='yolov8_phase2_fine', exist_ok=True\n",
    "    )\n",
    "print(\"--- ✅ PHASE 2 TRAINING COMPLETE ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Note\n",
    "- Ensure there’s no second os.rename after the move_dir_overwrite or within the context-managed block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'safe_rename' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msafe_rename\u001b[49m(labels_dir, labels_phase1_dir)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- ✅ PHASE 1 TRAINING COMPLETE ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m os\u001b[38;5;241m.\u001b[39mrename(labels_dir, labels_phase1_dir)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'safe_rename' is not defined"
     ]
    }
   ],
   "source": [
    "safe_rename(labels_dir, labels_phase1_dir)\n",
    "\n",
    "\n",
    "print(\"--- ✅ PHASE 1 TRAINING COMPLETE ---\")\n",
    "os.rename(labels_dir, labels_phase1_dir)\n",
    "\n",
    "# Phase 2 (transfer learning) - same flags; ensure phase2 labels are present\n",
    "print(\"\\n--- 🚀 STARTING PHASE 2 TRAINING ---\")\n",
    "os.rename(labels_phase2_dir, labels_dir) if os.path.exists(labels_phase2_dir) and not os.path.exists(labels_dir) else None\n",
    "\n",
    "model_p2 = YOLO('runs/detect/yolov8_phase1_coarse/weights/best.pt')\n",
    "model_p2.train(\n",
    "    data='phase2-data.yaml',\n",
    "    epochs=50,\n",
    "    imgsz=512,\n",
    "    batch=16,\n",
    "    workers=8,\n",
    "    device=device,\n",
    "    cache=True,\n",
    "    half=(device != 'cpu'),\n",
    "    patience=10,\n",
    "    name='yolov8_phase2_fine',\n",
    "    exist_ok=True\n",
    ")\n",
    "print(\"--- ✅ PHASE 2 TRAINING COMPLETE ---\")\n",
    "os.rename(labels_dir, labels_phase2_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- 🕵️‍♂️ RUNNING HIERARCHICAL PREDICTION ---\")\n",
    "\n",
    "# --- Load both trained models ---\n",
    "model_coarse = YOLO('runs/detect/yolov8_phase1_coarse/weights/best.pt')\n",
    "model_fine = YOLO('runs/detect/yolov8_phase2_fine/weights/best.pt')\n",
    "\n",
    "# --- Helper function for IoU ---\n",
    "def calculate_iou(box1, box2):\n",
    "    x1_inter = max(box1[0], box2[0])\n",
    "    y1_inter = max(box1[1], box2[1])\n",
    "    x2_inter = min(box1[2], box2[2])\n",
    "    y2_inter = min(box1[3], box2[3])\n",
    "    inter_area = max(0, x2_inter - x1_inter) * max(0, y2_inter - y1_inter)\n",
    "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "    return inter_area / union_area if union_area > 0 else 0\n",
    "\n",
    "# --- Select a random image to test ---\n",
    "test_image_name = random.choice(os.listdir(images_path))\n",
    "test_image_path = os.path.join(images_path, test_image_name)\n",
    "print(f\"-> Testing on image: {test_image_name}\")\n",
    "\n",
    "img = cv2.imread(test_image_path)\n",
    "results_coarse = model_coarse(img, verbose=False)[0]\n",
    "results_fine = model_fine(img, verbose=False)[0]\n",
    "IOU_THRESHOLD = 0.7\n",
    "\n",
    "for fine_box in results_fine.boxes:\n",
    "    fine_xyxy = fine_box.xyxy[0].cpu().numpy()\n",
    "    fine_class_name = model_fine.names[int(fine_box.cls)]\n",
    "    best_match_coarse_name = \"Unknown\"\n",
    "    max_iou = 0\n",
    "    \n",
    "    for coarse_box in results_coarse.boxes:\n",
    "        iou = calculate_iou(fine_xyxy, coarse_box.xyxy[0].cpu().numpy())\n",
    "        if iou > max_iou:\n",
    "            max_iou = iou\n",
    "            best_match_coarse_name = model_coarse.names[int(coarse_box.cls)]\n",
    "    \n",
    "    label = f\"{best_match_coarse_name}: {fine_class_name}\" if max_iou > IOU_THRESHOLD else fine_class_name\n",
    "        \n",
    "    x1, y1, x2, y2 = map(int, fine_xyxy)\n",
    "    cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "    cv2.putText(img, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "# --- Display the final result ---\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"Hierarchical Prediction Result\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "https://storage.googleapis.com/kaggle-colab-exported-notebooks/lfiathan/fashion-object-detection-yolov8.a6e234ab-f7a7-40f8-9140-5bad7f55cb2f.ipynb?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com/20251014/auto/storage/goog4_request&X-Goog-Date=20251014T010236Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=3978c138a741d01d0d9ed852961b8b655b604d0061e474f0688ff9ba4a21bbf2eb04cd49d373d97b40e8d1d5cc269cefbdc1e051818e4726a59ba9fbb07c544ce192f3c7a4b555756e524957f2cc0198805c8b44c7f6a9b5c54192199cc1074efba5fc8b8c470dc074498dca988d0419eb1cce84de375021f691043534f631c948d0156dce3b96d33f555bb9568e03ff247c6ff832ef844af02aac28a30e77e896ce72c2fa8e6c3b17d54b557c33f5bcf7ef21c2e41c26e0d6086ba2d49bcb76413f30dcdb5eac6187fb630a42dcb66cef313fb0337b56a128eef59234f33993e247604ba4909524778aa3a065c3ee7d7cae764fcdffce11f59e55ac46a7e506",
     "timestamp": 1760407058663
    }
   ]
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1942455,
     "sourceId": 3200379,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30408,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
